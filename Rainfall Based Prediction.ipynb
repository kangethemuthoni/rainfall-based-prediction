{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebd0ad6f-27ae-45c5-a0ff-5b6e9a81aab8",
   "metadata": {},
   "source": [
    "<font size=\"4\">**25. Load the Dataset**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6e52b3d-f019-4fc5-8ece-9fa111b7dd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Data:\n",
      "        Country  Year  Month  Rainfall\n",
      "0      DJIBOUTI  1981      1  0.000452\n",
      "1  ILE TROMELIN  1981      1  0.012166\n",
      "2     SWAZILAND  1981      1  0.023881\n",
      "3          MALI  1981      1  0.004452\n",
      "4         NIGER  1981      1  0.007719\n",
      "\n",
      "Dataset Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 322994 entries, 0 to 322993\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count   Dtype  \n",
      "---  ------    --------------   -----  \n",
      " 0   Country   322994 non-null  object \n",
      " 1   Year      322994 non-null  int64  \n",
      " 2   Month     322994 non-null  int64  \n",
      " 3   Rainfall  322994 non-null  float64\n",
      "dtypes: float64(1), int64(2), object(1)\n",
      "memory usage: 9.9+ MB\n",
      "None\n",
      "\n",
      "Missing Values:\n",
      "Country     0\n",
      "Year        0\n",
      "Month       0\n",
      "Rainfall    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#load the dataset\n",
    "file_path = './Downloads/final_rainfall_data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "#display dataset information\n",
    "print(\"Sample Data:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nDataset Information:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8e2b25-1794-479a-8127-faa65970f452",
   "metadata": {},
   "source": [
    "<font size=\"4\">**26. Data Cleaning**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aba918bb-08e3-4945-9c40-936213cc9317",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#fill any remaining NaN values in Rainfall with the column mean\n",
    "df['Rainfall'] = df['Rainfall'].fillna(df['Rainfall'].mean())\n",
    "\n",
    "#combine Year and Month into a Date column\n",
    "df['Date'] = pd.to_datetime(df[['Year', 'Month']].assign(Day=1))\n",
    "\n",
    "#normalize Rainfall\n",
    "scaler = MinMaxScaler()\n",
    "df['Rainfall'] = scaler.fit_transform(df[['Rainfall']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33593818-6ba6-47e6-acee-90487d0d4045",
   "metadata": {},
   "source": [
    "<font size=\"4\">**27. Feature Engineering**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd77ba6-32c1-4825-9c8f-83e7f1333a17",
   "metadata": {},
   "source": [
    "This section introduces feature engineering to enhance the rainfall dataset by adding lagged features, rolling averages, and cyclical month representations. Lag features are created to capture the rainfall from the previous 12 months for each country, helping to identify temporal dependencies. Using the groupby method ensures that lagged values are calculated independently for each country. Rolling averages over 3-month and 6-month windows are then computed to capture short-term and medium-term rainfall trends, providing smoothed representations of temporal patterns.\n",
    "\n",
    "To account for seasonality, cyclical features for months are generated using sine and cosine transformations. These transformations ensure that months like December and January, which are numerically far apart but seasonally close, are correctly represented in the dataset. Any missing values introduced by lagging or rolling averages are addressed using linear interpolation, while remaining NaN values are filled using a combination of backward and forward fill methods to ensure data completeness. These engineered features prepare the dataset for advanced time-series analysis or machine learning models, improving their ability to detect patterns and predict future outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64d0fc40-5dfc-4588-bab4-4e61f0785027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature-Engineered Data Sample:\n",
      "        Country  Year  Month  Rainfall       Date     Lag_1     Lag_2  \\\n",
      "0      DJIBOUTI  1981      1  0.016678 1981-01-01  0.006167  0.006167   \n",
      "1  ILE TROMELIN  1981      1  0.449310 1981-01-01  0.006167  0.006167   \n",
      "2     SWAZILAND  1981      1  0.881942 1981-01-01  0.006167  0.006167   \n",
      "3          MALI  1981      1  0.164401 1981-01-01  0.006167  0.006167   \n",
      "4         NIGER  1981      1  0.285065 1981-01-01  0.006167  0.006167   \n",
      "\n",
      "      Lag_3     Lag_4     Lag_5  ...     Lag_7     Lag_8     Lag_9    Lag_10  \\\n",
      "0  0.006167  0.006167  0.006167  ...  0.006167  0.006167  0.006167  0.006167   \n",
      "1  0.006167  0.006167  0.006167  ...  0.006167  0.006167  0.006167  0.006167   \n",
      "2  0.006167  0.006167  0.006167  ...  0.006167  0.006167  0.006167  0.006167   \n",
      "3  0.006167  0.006167  0.006167  ...  0.006167  0.006167  0.006167  0.006167   \n",
      "4  0.006167  0.006167  0.006167  ...  0.006167  0.006167  0.006167  0.006167   \n",
      "\n",
      "     Lag_11    Lag_12  Rolling_Mean_3  Rolling_Mean_6  Month_sin  Month_cos  \n",
      "0  0.006167  0.006167        0.011325        0.024201        0.5   0.866025  \n",
      "1  0.006167  0.006167        0.011325        0.024201        0.5   0.866025  \n",
      "2  0.006167  0.006167        0.011325        0.024201        0.5   0.866025  \n",
      "3  0.006167  0.006167        0.011325        0.024201        0.5   0.866025  \n",
      "4  0.006167  0.006167        0.011325        0.024201        0.5   0.866025  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bg/x64ncvbx1vv_c7_7d1fn_w8r0000gn/T/ipykernel_2048/1340721940.py:17: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df = df.interpolate(method='linear')\n",
      "/var/folders/bg/x64ncvbx1vv_c7_7d1fn_w8r0000gn/T/ipykernel_2048/1340721940.py:20: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df = df.fillna(method='bfill').fillna(method='ffill')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#generate lag features (previous 12 months of rainfall)\n",
    "for lag in range(1, 13):\n",
    "    df[f'Lag_{lag}'] = df.groupby('Country')['Rainfall'].shift(lag)\n",
    "\n",
    "#generate rolling averages\n",
    "df['Rolling_Mean_3'] = df.groupby('Country')['Rainfall'].transform(lambda x: x.rolling(window=3).mean())\n",
    "df['Rolling_Mean_6'] = df.groupby('Country')['Rainfall'].transform(lambda x: x.rolling(window=6).mean())\n",
    "\n",
    "#generate cyclical month features\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
    "df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
    "\n",
    "#fill NaNs caused by lagging and rolling averages using interpolation\n",
    "df = df.interpolate(method='linear')\n",
    "\n",
    "#fill any remaining NaN values with a fallback method\n",
    "df = df.fillna(method='bfill').fillna(method='ffill')\n",
    "\n",
    "print(\"\\nFeature-Engineered Data Sample:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0632ecf-b874-4254-9cf1-492b62bb4bb2",
   "metadata": {},
   "source": [
    "<font size=\"4\">**28. Define Target Variable**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3f204b-e054-4fd8-9fc1-209903738d5a",
   "metadata": {},
   "source": [
    "This section focuses on categorizing rainfall levels and encoding the categories for further analysis. Rainfall data is divided into three categories; Drought, Normal, and Flood, based on thresholds derived from the 25th and 75th percentiles (quantiles) of the Rainfall column. Rainfall below the lower threshold is categorized as likely to lead to Drought, above the upper threshold as likely to lead to a Flood, and values in between are labeled as Normal. This categorization allows for a simplified analysis of rainfall patterns and their extremes.\n",
    "\n",
    "Once categorized, the Rainfall_Category column is encoded into numerical labels using Scikit-learn's LabelEncoder. This transformation assigns a unique integer to each category, enabling machine learning algorithms to process the data efficiently. The resulting dataset includes both the categorical labels and their numerical representations, making it ready for predictive modeling or statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a8d18bf-d882-45ad-add1-555113d82a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Categorized Data Sample:\n",
      "   Rainfall Rainfall_Category  Rainfall_Category_Encoded\n",
      "0  0.016678            Normal                          2\n",
      "1  0.449310             Flood                          1\n",
      "2  0.881942             Flood                          1\n",
      "3  0.164401             Flood                          1\n",
      "4  0.285065             Flood                          1\n",
      "5  0.093802            Normal                          2\n",
      "6  0.068182            Normal                          2\n",
      "7  0.000000           Drought                          0\n",
      "8  0.077412            Normal                          2\n",
      "9  0.104673            Normal                          2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#define thresholds\n",
    "drought_threshold = df['Rainfall'].quantile(0.25)\n",
    "flood_threshold = df['Rainfall'].quantile(0.75)\n",
    "\n",
    "#categorize rainfall\n",
    "def categorize_rainfall(rainfall):\n",
    "    if rainfall < drought_threshold:\n",
    "        return 'Drought'\n",
    "    elif rainfall > flood_threshold:\n",
    "        return 'Flood'\n",
    "    else:\n",
    "        return 'Normal'\n",
    "\n",
    "df['Rainfall_Category'] = df['Rainfall'].apply(categorize_rainfall)\n",
    "\n",
    "#encode labels\n",
    "encoder = LabelEncoder()\n",
    "df['Rainfall_Category_Encoded'] = encoder.fit_transform(df['Rainfall_Category'])\n",
    "\n",
    "print(\"\\nCategorized Data Sample:\")\n",
    "print(df[['Rainfall', 'Rainfall_Category', 'Rainfall_Category_Encoded']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa034cfe-0b8b-450f-8d09-ac00348549c6",
   "metadata": {},
   "source": [
    "<font size=\"4\">**29. Encode Categorical Variables**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f16d92-ac03-46c4-b142-67d4cfb1089a",
   "metadata": {},
   "source": [
    "This section demonstrates the use of one-hot encoding to convert the categorical Country column into numerical features suitable for machine learning models. The OneHotEncoder from Scikit-learn is applied with the drop='first' option, which prevents multicollinearity by omitting the first category from the encoded features. This creates binary columns representing the presence or absence of each country in the dataset.\n",
    "\n",
    "The encoded features are stored in a new DataFrame, where each column corresponds to a unique country, prefixed with Country_. The original index of the dataset is preserved to ensure alignment with the existing data. The encoded features are then concatenated with the original DataFrame after dropping the Country column. This results in a dataset where the categorical Country data is replaced with meaningful binary features, improving compatibility with machine learning algorithms that require numerical inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0229570-d3a6-4be4-b955-a90f730bfb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#apply one-hot encoding to the Country column\n",
    "encoder_onehot = OneHotEncoder(sparse_output=False, drop='first')\n",
    "encoded_countries = encoder_onehot.fit_transform(df[['Country']])\n",
    "\n",
    "#create DataFrame for encoded features\n",
    "country_columns = [f'Country_{cat}' for cat in encoder_onehot.categories_[0][1:]]\n",
    "encoded_countries_df = pd.DataFrame(encoded_countries, columns=country_columns, index=df.index)\n",
    "\n",
    "#concatenate encoded features\n",
    "df = pd.concat([df.drop('Country', axis=1), encoded_countries_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff07386a-34eb-4205-be81-86a709c57cb0",
   "metadata": {},
   "source": [
    "<font size=\"4\">**30. Split Data into Training and Testing Sets**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753facbc-92ff-42a3-8228-056f2e53a3f4",
   "metadata": {},
   "source": [
    "This section prepares the dataset for machine learning by splitting it into training and testing sets. The feature set (X) is created by dropping irrelevant columns, including Rainfall_Category (the categorical version of the target), Rainfall_Category_Encoded (the target variable itself), and Date (a temporal identifier not needed for training). The target variable (y) is defined as the encoded rainfall category (Rainfall_Category_Encoded).\n",
    "\n",
    "The dataset is split using Scikit-learn’s train_test_split function, which ensures that 20% of the data is reserved for testing while 80% is used for training the model. The stratify parameter is set to y to maintain the same class distribution in both the training and testing sets, preventing imbalances in the target variable across splits. The final split sizes are printed to confirm the dimensions of the training and testing sets, ensuring the data is ready for building and evaluating machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4606ce59-092c-4c2a-9e5f-60939842bf62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: (258395, 78), Testing Set: (64599, 78)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#features and target variable\n",
    "X = df.drop(['Rainfall_Category', 'Rainfall_Category_Encoded', 'Date'], axis=1)\n",
    "y = df['Rainfall_Category_Encoded']\n",
    "\n",
    "#stratified splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"Training Set: {X_train.shape}, Testing Set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed67f151-6bf6-4df2-8b44-51ecffab5024",
   "metadata": {},
   "source": [
    "<font size=\"4\">**31. Multilayer Perceptron (MLP) for Rainfall Classification**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f77af1f-815e-4d3a-a447-b5f145d032ef",
   "metadata": {},
   "source": [
    "This section demonstrates the implementation of a Multilayer Perceptron (MLP), a deep learning model, to classify rainfall data into categories such as Drought, Normal, and Flood. The input features are standardized using StandardScaler to normalize the range of values, ensuring efficient model training and faster convergence. The MLP is designed using the Sequential API in Keras, with three fully connected (Dense) hidden layers, each using ReLU activation to introduce non-linearity. Dropout layers (30%) are incorporated after the first and second hidden layers to mitigate overfitting by randomly deactivating neurons during training.\n",
    "\n",
    "The output layer employs a softmax activation function, suitable for multi-class classification tasks, and the number of neurons equals the number of unique target classes. The model is compiled using the Adam optimizer, which adapts learning rates dynamically for efficient updates, and sparse categorical cross-entropy loss, appropriate for integer-labeled multi-class targets. Training is performed for 5 epochs with a batch size of 32, reserving 20% of the training data for validation. Model performance is evaluated using accuracy and a classification report, which provide detailed insights into precision, recall, and F1-scores for each rainfall category. This MLP effectively captures patterns in the data, enabling accurate rainfall classification for drought and flood prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea01ac19-2fd1-4fce-ba0b-78d9ac56b5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 493us/step - accuracy: 0.8666 - loss: 0.3168 - val_accuracy: 0.9547 - val_loss: 0.1035\n",
      "Epoch 2/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 481us/step - accuracy: 0.9490 - loss: 0.1227 - val_accuracy: 0.9690 - val_loss: 0.0807\n",
      "Epoch 3/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 479us/step - accuracy: 0.9599 - loss: 0.0983 - val_accuracy: 0.9710 - val_loss: 0.0666\n",
      "Epoch 4/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 483us/step - accuracy: 0.9639 - loss: 0.0882 - val_accuracy: 0.9736 - val_loss: 0.0614\n",
      "Epoch 5/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 482us/step - accuracy: 0.9669 - loss: 0.0807 - val_accuracy: 0.9768 - val_loss: 0.0595\n",
      "Epoch 6/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 497us/step - accuracy: 0.9692 - loss: 0.0764 - val_accuracy: 0.9788 - val_loss: 0.0515\n",
      "Epoch 7/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 479us/step - accuracy: 0.9707 - loss: 0.0715 - val_accuracy: 0.9824 - val_loss: 0.0467\n",
      "Epoch 8/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 488us/step - accuracy: 0.9727 - loss: 0.0677 - val_accuracy: 0.9817 - val_loss: 0.0506\n",
      "Epoch 9/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 483us/step - accuracy: 0.9732 - loss: 0.0660 - val_accuracy: 0.9794 - val_loss: 0.0450\n",
      "Epoch 10/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 502us/step - accuracy: 0.9742 - loss: 0.0638 - val_accuracy: 0.9779 - val_loss: 0.0547\n",
      "Epoch 11/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 486us/step - accuracy: 0.9753 - loss: 0.0623 - val_accuracy: 0.9778 - val_loss: 0.0489\n",
      "Epoch 12/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 517us/step - accuracy: 0.9754 - loss: 0.0613 - val_accuracy: 0.9764 - val_loss: 0.0496\n",
      "Epoch 13/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 510us/step - accuracy: 0.9765 - loss: 0.0589 - val_accuracy: 0.9811 - val_loss: 0.0429\n",
      "Epoch 14/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 498us/step - accuracy: 0.9775 - loss: 0.0563 - val_accuracy: 0.9835 - val_loss: 0.0367\n",
      "Epoch 15/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 490us/step - accuracy: 0.9782 - loss: 0.0547 - val_accuracy: 0.9830 - val_loss: 0.0401\n",
      "Epoch 16/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 490us/step - accuracy: 0.9776 - loss: 0.0558 - val_accuracy: 0.9809 - val_loss: 0.0428\n",
      "Epoch 17/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 507us/step - accuracy: 0.9785 - loss: 0.0537 - val_accuracy: 0.9865 - val_loss: 0.0335\n",
      "Epoch 18/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 531us/step - accuracy: 0.9798 - loss: 0.0517 - val_accuracy: 0.9801 - val_loss: 0.0433\n",
      "Epoch 19/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 500us/step - accuracy: 0.9791 - loss: 0.0521 - val_accuracy: 0.9823 - val_loss: 0.0425\n",
      "Epoch 20/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 505us/step - accuracy: 0.9799 - loss: 0.0494 - val_accuracy: 0.9847 - val_loss: 0.0368\n",
      "Epoch 21/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 495us/step - accuracy: 0.9803 - loss: 0.0490 - val_accuracy: 0.9778 - val_loss: 0.0481\n",
      "Epoch 22/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 505us/step - accuracy: 0.9808 - loss: 0.0479 - val_accuracy: 0.9864 - val_loss: 0.0326\n",
      "Epoch 23/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 514us/step - accuracy: 0.9811 - loss: 0.0480 - val_accuracy: 0.9821 - val_loss: 0.0385\n",
      "Epoch 24/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 490us/step - accuracy: 0.9806 - loss: 0.0487 - val_accuracy: 0.9883 - val_loss: 0.0331\n",
      "Epoch 25/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 484us/step - accuracy: 0.9811 - loss: 0.0473 - val_accuracy: 0.9857 - val_loss: 0.0337\n",
      "Epoch 26/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 490us/step - accuracy: 0.9823 - loss: 0.0449 - val_accuracy: 0.9866 - val_loss: 0.0335\n",
      "Epoch 27/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 483us/step - accuracy: 0.9824 - loss: 0.0440 - val_accuracy: 0.9837 - val_loss: 0.0365\n",
      "Epoch 28/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 515us/step - accuracy: 0.9826 - loss: 0.0438 - val_accuracy: 0.9842 - val_loss: 0.0340\n",
      "Epoch 29/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 482us/step - accuracy: 0.9819 - loss: 0.0455 - val_accuracy: 0.9846 - val_loss: 0.0366\n",
      "Epoch 30/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 483us/step - accuracy: 0.9829 - loss: 0.0445 - val_accuracy: 0.9849 - val_loss: 0.0340\n",
      "\u001b[1m2019/2019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 262us/step\n",
      "Accuracy: 0.9857\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Drought       0.96      0.99      0.98     16150\n",
      "      Normal       1.00      0.99      1.00     16150\n",
      "       Flood       0.99      0.98      0.99     32299\n",
      "\n",
      "    accuracy                           0.99     64599\n",
      "   macro avg       0.98      0.99      0.99     64599\n",
      "weighted avg       0.99      0.99      0.99     64599\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "#scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#define the deep learning model\n",
    "model = Sequential([\n",
    "    Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(y_train.unique()), activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "#compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#train the model\n",
    "history = model.fit(X_train_scaled, y_train,\n",
    "                    validation_split=0.2,\n",
    "                    epochs=30,\n",
    "                    batch_size=32,\n",
    "                    verbose=1)\n",
    "\n",
    "#evaluate the model\n",
    "y_pred_probs = model.predict(X_test_scaled)\n",
    "y_pred = tf.argmax(y_pred_probs, axis=1).numpy()\n",
    "\n",
    "#classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=['Drought', 'Normal', 'Flood'])\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7516fa6c-4b65-4891-8f5e-a5e8ddd66b9a",
   "metadata": {},
   "source": [
    "<font size=\"4\">**32. LSTM Model for Rainfall Classification**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2468937e-79e1-43e1-8968-2025fb4283a0",
   "metadata": {},
   "source": [
    "This section employs a Long Short-Term Memory (LSTM) network to classify rainfall data into categories (Drought, Normal, and Flood). Although the dataset is not inherently sequential, the input data is reshaped into a 3D format—required for LSTM layers—with a single timestep (timesteps=1). Before reshaping, the features are scaled using StandardScaler to normalize the data, ensuring consistency and faster model convergence. The reshaped input allows the LSTM to process data in a sequence-like format, capturing potential patterns across features.\n",
    "\n",
    "The model architecture includes an LSTM layer with 50 units and ReLU activation, followed by a Dense layer with 32 neurons for additional feature extraction. The output layer uses a softmax activation function for multi-class classification, with the number of neurons matching the target classes. Compiled with the Adam optimizer and sparse categorical cross-entropy loss, the model is trained for 5 epochs with a batch size of 32, reserving 20% of the training data for validation. Model performance is evaluated on the test set using accuracy and a classification report, which provide insights into its effectiveness in distinguishing between rainfall categories. This approach leverages LSTM's capability to capture patterns, even in reshaped static data, to improve classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c24c8d64-4c4e-41b2-b99b-fe8239b1063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#assume X_train and X_test are the original datasets\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#reshape data into 3D format: (samples, timesteps, features)\n",
    "#for example, using 1 timestep if not dealing with time-series\n",
    "X_train_reshaped = np.expand_dims(X_train_scaled, axis=1)  # Shape: (samples, timesteps=1, features)\n",
    "X_test_reshaped = np.expand_dims(X_test_scaled, axis=1)    # Shape: (samples, timesteps=1, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ffdbe85-fa6e-4690-91fa-868a5d4683a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 602us/step - accuracy: 0.9014 - loss: 0.2441 - val_accuracy: 0.9706 - val_loss: 0.0674\n",
      "Epoch 2/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 603us/step - accuracy: 0.9711 - loss: 0.0708 - val_accuracy: 0.9760 - val_loss: 0.0579\n",
      "Epoch 3/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 596us/step - accuracy: 0.9757 - loss: 0.0584 - val_accuracy: 0.9768 - val_loss: 0.0539\n",
      "Epoch 4/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 586us/step - accuracy: 0.9787 - loss: 0.0513 - val_accuracy: 0.9777 - val_loss: 0.0540\n",
      "Epoch 5/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 585us/step - accuracy: 0.9791 - loss: 0.0493 - val_accuracy: 0.9825 - val_loss: 0.0416\n",
      "Epoch 6/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 603us/step - accuracy: 0.9815 - loss: 0.0447 - val_accuracy: 0.9825 - val_loss: 0.0424\n",
      "Epoch 7/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 589us/step - accuracy: 0.9822 - loss: 0.0425 - val_accuracy: 0.9843 - val_loss: 0.0388\n",
      "Epoch 8/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 603us/step - accuracy: 0.9833 - loss: 0.0394 - val_accuracy: 0.9767 - val_loss: 0.0520\n",
      "Epoch 9/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 588us/step - accuracy: 0.9842 - loss: 0.0380 - val_accuracy: 0.9820 - val_loss: 0.0423\n",
      "Epoch 10/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 598us/step - accuracy: 0.9846 - loss: 0.0376 - val_accuracy: 0.9847 - val_loss: 0.0367\n",
      "Epoch 11/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 614us/step - accuracy: 0.9853 - loss: 0.0352 - val_accuracy: 0.9865 - val_loss: 0.0353\n",
      "Epoch 12/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 589us/step - accuracy: 0.9863 - loss: 0.0342 - val_accuracy: 0.9857 - val_loss: 0.0348\n",
      "Epoch 13/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 601us/step - accuracy: 0.9862 - loss: 0.0336 - val_accuracy: 0.9864 - val_loss: 0.0370\n",
      "Epoch 14/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 592us/step - accuracy: 0.9860 - loss: 0.0336 - val_accuracy: 0.9881 - val_loss: 0.0276\n",
      "Epoch 15/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 605us/step - accuracy: 0.9862 - loss: 0.0328 - val_accuracy: 0.9908 - val_loss: 0.0257\n",
      "Epoch 16/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 605us/step - accuracy: 0.9868 - loss: 0.0314 - val_accuracy: 0.9867 - val_loss: 0.0319\n",
      "Epoch 17/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 597us/step - accuracy: 0.9870 - loss: 0.0312 - val_accuracy: 0.9885 - val_loss: 0.0295\n",
      "Epoch 18/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 601us/step - accuracy: 0.9870 - loss: 0.0306 - val_accuracy: 0.9872 - val_loss: 0.0305\n",
      "Epoch 19/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 604us/step - accuracy: 0.9878 - loss: 0.0290 - val_accuracy: 0.9891 - val_loss: 0.0283\n",
      "Epoch 20/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 598us/step - accuracy: 0.9878 - loss: 0.0290 - val_accuracy: 0.9830 - val_loss: 0.0370\n",
      "Epoch 21/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 601us/step - accuracy: 0.9880 - loss: 0.0284 - val_accuracy: 0.9890 - val_loss: 0.0265\n",
      "Epoch 22/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 608us/step - accuracy: 0.9887 - loss: 0.0277 - val_accuracy: 0.9821 - val_loss: 0.0394\n",
      "Epoch 23/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 615us/step - accuracy: 0.9882 - loss: 0.0277 - val_accuracy: 0.9862 - val_loss: 0.0335\n",
      "Epoch 24/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 608us/step - accuracy: 0.9890 - loss: 0.0266 - val_accuracy: 0.9853 - val_loss: 0.0385\n",
      "Epoch 25/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 612us/step - accuracy: 0.9889 - loss: 0.0275 - val_accuracy: 0.9887 - val_loss: 0.0299\n",
      "Epoch 26/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 602us/step - accuracy: 0.9893 - loss: 0.0262 - val_accuracy: 0.9898 - val_loss: 0.0270\n",
      "Epoch 27/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 615us/step - accuracy: 0.9894 - loss: 0.0259 - val_accuracy: 0.9889 - val_loss: 0.0292\n",
      "Epoch 28/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 619us/step - accuracy: 0.9897 - loss: 0.0250 - val_accuracy: 0.9900 - val_loss: 0.0251\n",
      "Epoch 29/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 611us/step - accuracy: 0.9899 - loss: 0.0247 - val_accuracy: 0.9892 - val_loss: 0.0268\n",
      "Epoch 30/30\n",
      "\u001b[1m6460/6460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 604us/step - accuracy: 0.9898 - loss: 0.0246 - val_accuracy: 0.9879 - val_loss: 0.0292\n",
      "\u001b[1m2019/2019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 331us/step\n",
      "Accuracy: 0.9874\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Drought       0.97      0.99      0.98     16150\n",
      "      Normal       1.00      0.99      0.99     16150\n",
      "       Flood       0.99      0.99      0.99     32299\n",
      "\n",
      "    accuracy                           0.99     64599\n",
      "   macro avg       0.99      0.99      0.99     64599\n",
      "weighted avg       0.99      0.99      0.99     64599\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#define the LSTM model\n",
    "lstm_model = Sequential([\n",
    "    LSTM(50, activation='relu', input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(y_train.unique()), activation='softmax')  # Output layer for multi-class classification\n",
    "])\n",
    "\n",
    "#compile the model\n",
    "lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = lstm_model.fit(\n",
    "    X_train_reshaped, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "#evaluate the model\n",
    "y_pred_probs = lstm_model.predict(X_test_reshaped)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "#classification report\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=['Drought', 'Normal', 'Flood'])\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b00c36-89cf-48e1-ad2f-560bb936d38d",
   "metadata": {},
   "source": [
    "<font size=\"4\">**33. Save Models**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b87ad465-0b4b-4174-8335-3c45ebb437dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep learning model saved as models/deep_learning_model2.h5\n"
     ]
    }
   ],
   "source": [
    "#save the deep learning model as .h5\n",
    "deep_learning_filename_h5 = \"models/deep_learning_model2.h5\"\n",
    "model.save(deep_learning_filename_h5)\n",
    "print(f\"Deep learning model saved as {deep_learning_filename_h5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52e5d54e-d7cb-45fa-a086-ef9dfeb457b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM model saved as models/lstm_model2.h5\n"
     ]
    }
   ],
   "source": [
    "#save the LSTM model as .h5\n",
    "lstm_model_filename_h5 = \"models/lstm_model2.h5\"\n",
    "model.save(lstm_model_filename_h5)\n",
    "print(f\"LSTM model saved as {lstm_model_filename_h5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3a9d36-e846-44d0-b1ac-b9695897a980",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
